import re
GROUPING_SPACE_REGEX = re.compile(r'([^\w]|[+])', re.UNICODE)
def simple_word_tokenize(text, _split=GROUPING_SPACE_REGEX.split):
    return [t for t in _split(text.lower()) if len(t)>1 and not t.isspace()]
    
    
def token_r(text):
    words = simple_word_tokenize(text)
    return [porter_stemmer.stem(x) for x in words if len(x)>=2]
